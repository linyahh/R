---
title: " VAST Challenge 2021 -Mini-Challenge 3"
description: |
  A short description of the post.
author:
  - name: Linya Huang
    url: https://www.linkedin.com/in/linya-huang/
date: 07-03-2021
output:
  distill::distill_article:
    toc: true
    toc_depth: 6
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3,
  echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning=FALSE)
```

# 1.Introduction



# 2.Literature Review



# 3.Data Preprocessing and Exploratory

## 3.1 Import packages and social media stream data

```{r}
packages= c('raster','sf','clock','tmap',
            'tidyverse','data.table','lubridate',
            'textclean','tm','wordcloud','wordcloud2','text2vec',
            'topicmodels','tidytext','textmineR','quanteda',
            'BTM','textplot','concaveman','ggwordcloud',
            'qdapDictionaries','textstem')

for(p in packages){
  if(!require(p,character.only= T)){
    install.packages(p)
    }
  library(p, character.only = T)
}
```
```{r}
#install.packages("devtools")
library(devtools)
#install_github("cbail/textnets")
library(textnets)
```



```{r}
#read csv file
data_17_1830=read_csv("data/MC3/csv-1700-1830.csv")
data_1830_20=read_csv("data/MC3/csv-1831-2000.csv")
data_20_2130=read_csv("data/MC3/csv-2001-2131.csv")

#append data

data=rbindlist(list(data_17_1830,data_1830_20,data_20_2130))

#print head of data
head(data)

```
 

## 3.2 Change date type

```{r}

#timestamp in lubridate
data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)

```

## Data Exploratory
  
  •1. Number of post throughout the time period
  
  
```{r}
data$time_1min = cut(data$timestamp, breaks="1 min")
 

```
```{r}

# ggplot(count,aes(x=time_1min,y=count_of_posts,fill=type))+
#          geom_bar(stat="identity",position="dodge")+
#   theme(axis.text.x = element_text(angle = 90, hjust = 1))+
#   ggtitle("Total Number of Posts through the period")


# ggplot(count) + 
#   geom_point(aes(x=time_1min,y=count_of_posts,colour = type), size = 3) +
#   geom_line(data = count, aes(x=time_1min,y=count_of_posts))


# ggplot(count, aes(x=time_1min,y=count_of_posts,colour = type)) +
#   geom_point(size = 2.2) +
#   geom_line(data = count, aes(x=time_1min,y=count_of_posts,colour = type), size = 0.8, colour = 'red')

```

```{r}
head(count[order(-count$count_of_posts),],5)
```
 

 
 
 
## Text Data Preprocessing

```{r}


data$cleaned<-
  tolower(data$message)%>%   # transform all message to lower cases
  replace_contraction()%>%   #replace contractions with long form
  replace_word_elongation()%>% #remove the same letter (case insensitive) appears 3 times consecutively
  str_replace_all("[0-9]", "") %>% #removing numbers
  str_replace_all("([,=!.?$+%-&#@])","")%>% #remove punctuations
  #str_replace_all(c("rt|#hi|#pok|#pokrally|
  #                #abilapost|#kronosstar|#centralbulletin|@centralbulletin|@kronosstar|rally|aliba"),"")%>% #remove hashtag and rt
  removeWords(stopwords("english"))%>% 
  str_squish()%>% #trim whitespace from a string 
  str_trim %>% 
  lemmatize_strings()#removes whitespace from start and end of string


head(subset(data,select=c("message","cleaned")))


```

```{r}
data%>% filter(data$cleaned=="")
```


 
```{r}

#convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(data$cleaned)))

inspect(docs[1:2])

# Create a document-term-matrix

dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 

# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)

#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#wordcloud2(data=df, size=1.6, color='random-dark')

```


```{r}

#as_datetime(data$time_1min)
data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time_30min = cut(data$timestamp, breaks="30 min")
data$id <- seq.int(nrow(data))

data_subset=subset(data,select=c("time_30min","cleaned"))

```

```{r}
words_count<-data_subset%>% 
  unnest_tokens(word, cleaned) %>%
  count(time_30min,word, sort = TRUE)
```



```{r}
#tf_idf <- words_count %>% 
#  bind_tf_idf(word,id,n) %>% 
#  arrange(desc(tf_idf))


#tf_idf %>% 
#  filter(str_detect(newsgroup,"^sci\\.")) %>% 
#  group_by(newsgroup) %>% 
#  slice_max(tf_idf,
#            n=12) %>% 
#  ungroup() %>% 
#  mutate(word = reorder(word,
 #                        tf_idf)) %>% 
#  ggplot(aes(tf_idf,
#             word,
#             fill=newsgroup))+
 # geom_col(show.legend = FALSE)+
#  facet_wrap(~newsgroup,
#             scales="free")+
 # labs(x="tf-idf",
#       y=NULL)
```

# 4.Data Visualization for Challenge Questions



## 4.1 Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.


First the data is split by 30 minutes time interval and perform wordcloud, to visualize the most frequent words in the microblog.


```{r}

data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time_30min = cut(data$timestamp, breaks="30 min")
data$id <- seq.int(nrow(data))

data_subset=subset(data,select=c("time_30min","cleaned"))


usenet_words<-data_subset%>%
  group_by(time_30min) %>% 
  unnest_tokens(word, cleaned) %>%
  count(time_30min,word, sort = TRUE)


usenet_words[order(usenet_words$time_30min),]


set.seed(1234)
usenet_words %>%
  filter(n > 5&n<70) %>%
ggplot(aes(label = word,
           size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~time_30min)
```

From the visualization, we can tell from 1830-1900 fire at dancing dolphin becomes hot topic, then followed by shooting/cops/police at 1930-2000. And from 2100 words like fire/hostage/van/suspects/explosion were widely discussed online after 2100.








Then, TF-IDF 
```{r}
tf_idf <- usenet_words%>%
  bind_tf_idf(word,time_30min, n) %>%
  arrange(desc(tf_idf))

tf_idf %>%
  group_by(time_30min) %>%
  slice_max(tf_idf, 
            n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, 
                        tf_idf)) %>%
  ggplot(aes(tf_idf, 
             word, 
             fill = time_30min)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ time_30min, 
             scales = "free") +
  labs(x = "tf-idf", 
       y = NULL)
```


```{r}

library(qdapDictionaries)
is.word  <- function(x) x %in% GradyAugmented

tf_idf$is_word<-is.word(tf_idf$word)

tf_idf %>% filter(is_word=="FALSE")


tf_idf[order(tf_idf$n),] %>% filter(is_word=="FALSE")
```



Then, to distinguish meaningful events from chatter/junk/spam messages, I will then perform topic modeling to identify the topic in the microblogs.


```{r}
wordcorpus <- Corpus(VectorSource(as.character(data$cleaned)))  
dtm <- DocumentTermMatrix(wordcorpus,
                          control = list(
                            wordLengths=c(2, Inf),               # limit word length
                            bounds = list(global = c(1,Inf)),    # minimum word frequency
                            removeNumbers = TRUE,                #remove Numbers
                            weighting = weightTf,                #weighted term frequency
                            encoding = "UTF-8"))

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove 0 dtm rows of matrix

topic=LDA(dtm.new,k=10,method="Gibbs",conrol=list(seed=2021,alpha=0.01,iter=200))
```




Top five words in each topics

```{r}
terms(topic,5)

```

Extract per-topic-per-word probabilities ,β(“beta”), from the model. The higher the value, the more important of the words to the topic.


```{r}
ap_topics <- tidy(topic, matrix = "beta")


ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
As shown above, the meaningful event are in topic 1 - police relatedfire at dancing dolphin,topic 2- hostage & injure, topic 7 - van related, topic 10 police and scene related. While for other key words in the topic, we cann

```{r}
topic_gamma <- tidy(topic, matrix = "gamma")
topic_gamma <- topic_gamma %>% 
  group_by(document) %>% 
  slice(which.max(gamma))

topic_gamma$document<-as.numeric(topic_gamma$document)
topic_gamma[order(topic_gamma$document),]

```




```{r}
#Tokenize data
tidytxtdata<- tidy(dtm)
                    
tidytxtdata <- tidytxtdata%>% #Remove the count column
  select(-count)
tidytxtdata <- tidytxtdata%>% #Change the column name 'term' to 'word' so that we can get rid of stopwords later
  rename(word = term)


#Remove stopwords
tidytxtdata <- tidytxtdata%>%
  anti_join(stop_words)


#Use the btm model
set.seed(321)
model <- BTM(tidytxtdata, k = 20, beta = 0.01, background = TRUE, iter = 500, trace = 100) #Run the model

topicterms <- terms(model, top_n = 10) #View the topics
topicterms
```
```{r}
library(textplot)
library(ggraph)
library(concaveman)
plot(model)
```



#4.2 Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.



##Number of posts 

```{r}
count <- data %>%
  group_by(type,time_1min) %>%
  summarise(count_of_posts= n_distinct(message))

count$time_1min=ymd_hms(count$time_1min)
#count$time_1min=format(count$time_1min,format = "%H:%M:%S") 

mean=mean(count$count_of_posts)

ggplot(count,aes(x=time_1min,y=count_of_posts,fill=type))+
         geom_bar(stat="identity",position="dodge")+
  # geom_abline(h=mean, col = "black")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  
  ggtitle("Total Number of Posts through the period")
```
With time interval at 1 minute, the number of posts trend is plotted as above. The peaks of both ccdata and mbdata are observed during time 19:40-19:50, 18:45:00 and 20:10:00. 




```{r}
head(count[order(-count$count_of_posts),],10)
```


##Number of retweets/tweets

```{r}
data_rt2<-data %>% 
   #filter(str_detect(message, "fire")) %>% 
  select(c("author","time_1min","message","RT")) %>% 
  group_by(time_1min) %>% 
  summarise(post=n(),
            rt_post=sum(RT!=""))

data_rt2$time_1min=ymd_hms(data_rt2$time_1min)


#ggplot(fire,aes(x=time_1min,y=n))+
         #geom_bar(stat="identity",position="dodge")+
  #theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  #ggtitle("Total Number of Posts through the period")


ggplot(data_rt2,aes(x=time_1min)) +
    geom_bar(aes(y=post), stat = "identity",fill = "red") +
    geom_bar(aes(y=rt_post), stat = "identity",fill = "blue") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))+
    ggtitle("Tweet and RT")

```
Another way to detect the active level of microblog is to visualize the proportion of re-tweet/tweet. As shown in the bar graph above, the peak trend coincides with "Total Number of Posts through the period", indicating the people are actively eveloved in the events happening in Abila.

The goverment can monitor the ratio of retweet in community to detect abnormal event.

##Important news source - Mainstream media/ ccdata /KOL

Noticed that the mainstream media (with name starting with capital letter and frequently re-tweet by public) and call center are official media for Abila.

```{r}
# check if the media is mainstream media

data_RT %>% 
  filter(RT_from!=author) %>% 
  count(RT_from) %>% 
  arrange(desc(n)) %>% 
  filter(n>10) # filter the retweet over 10 times by public

```

@HomelandIlluminations/@AbilaPost/@KronosStar/@CentralBulletin/@NewsOnlineToday are top5 social media in area of Abila. Their tweet should be closely moniored by government, in particular, when the frequency of re-tweet from the public has rised.

For example for HomelandIlluminations, we can monitor the words and its relationships in re-tweets.

```{r}
#convert dataframe to corpus

library(udpipe)
library(igraph)
library(ggraph)
library(ggplot2)

data_RT$id <- seq.int(nrow(data_RT))
data_RT_subset<-data_RT %>% select(c("id","RT_message"))
data_RT_subset$cleaned <- str_replace_all(data_RT_subset$RT_message, 'rt ', '')

x<-data_RT_subset %>% 
  unnest_tokens(word, RT_message)

x<-cooccurrence(x, group = "id", term = "word")

plt <- textplot_cooccurrence(x,
                             title = "Re-tweet Co-occurrences", top_n = 25)
  
plt
```




tf-idf from call center


```{r}
data %>% filter(author=="FriendsOfKronos")
```




##the most popular retweets – word cloud// textnets
##real-time co-occurrence of texts








At 18:25:00, user GreyCatCollectibles first posted "Smell something funny upstairs - wish this renovation work would get finished!", followed by his post"Yuck - electrical smell is getting worse","OMG -- really smells like smoke now" in 10 minutes, making him the first person detecting the fire at dancing dolphin.

```{r}
#author_post<-data %>% count(author)
#author_post[order(author_post$n),]
```

```{r}
data %>% 
   filter(author=="GreyCatCollectibles") %>% 
  select(c("author","timestamp","message"))

```










  
```{r}

head(fire)
```

retrive RT from message
```{r}
regex <- "RT @([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
regex2 <- "@([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
regex3 <- "RT @([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.) "
data$RT_pattern<-str_extract_all(data$message, regex, simplify = TRUE)
data$RT_from<-  str_extract_all(data$RT_pattern, regex2, simplify = TRUE)
data$RT_message <- str_replace_all(data$message,regex3,"")
```

```{r}
data_RT<- data %>% 
  filter(RT_from!="")

head(data_RT)
```

```{r}
data_RT %>% 
  count(RT_message) %>% 
  arrange(desc(n))

```





```{r}
nrow(data_RT)
length(unique(data_RT[["message"]]))

#data_RT<-data_RT %>% select(c("author","message","RT_from"))


RT_edges_aggregated <- data_RT %>%
  group_by(RT_from,author) %>%
    summarise(Weight = n()) %>%
  filter(Weight > 1) %>%
  ungroup()

data_RT


RT_graph <- tbl_graph(
                           edges = RT_edges_aggregated, 
                           directed = TRUE)

```

```{r}
g <- ggraph(RT_graph, 
            layout = "nicely") + 
  geom_edge_link(aes()) +
  geom_node_point(aes(size = 3))
g + theme_graph()
```


```{r}
data_RT %>% 
  group_by(RT_from,RT_message,author) %>% 
  count(timestamp) %>% 
  ungroup()


RT_edges_aggregated <- data_RT %>%
  group_by(message,author) %>%
    summarise(Weight = n()) %>%
  filter(Weight >= 1) %>%
  ungroup()

RT_edges_aggregated 


RT_graph <- tbl_graph(
                           edges = RT_edges_aggregated, 
                           directed = TRUE)

g <- ggraph(RT_graph, 
            layout = "nicely") + 
  geom_edge_link(aes()) +
  geom_node_point(aes(size = 3))
g + theme_graph()
  
```

messages from call center 


```{r}
packages= c('raster','sf','clock','tmap','tidyverse')

for(p in packages){
  if(!require(p,character.only= T)){
    install.packages(p)
    }
  library(p, character.only = T)
}
```


```{r}
bgmap<- raster("data/MC3/Geospatial/MC2-tourist.tif")
bgmap

tmap_mode("plot") #interactive view

tm_shape(bgmap) +
  tm_rgb(bgmap, r=1, g=2, b=3,
         alpha=NA,
         saturation = 1,
         interpolate = TRUE,
         max.value = 255)
```


```{r}
cc_data<-data %>% 
  filter(type=="ccdata")

docs <- Corpus(VectorSource(as.character(cc_data$cleaned)))

inspect(docs[1:2])

# Create a document-term-matrix

dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 

# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)

#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))


```

```{r}
write.csv(data,"data/data.csv")
```



Then, frequency of the author

```{r}
groupby_author<-data %>% 
  group_by(author) %>% 
  count %>% 
  ungroup
```



```{r}
data_2<-data %>%
     group_by(author) %>%
     dplyr::summarise(all_posts=paste(message, collapse=" "))

sotu_firsts_nouns <- PrepText(data_2, groupvar = "author", textvar = "all_posts", node_type = "groups", tokenizer = "tweets", pos = "nouns", remove_stop_words = FALSE, compound_nouns = TRUE)

# <- PrepTextSent(data_2, groupvar = "author", textvar = "cleaned", node_type = "groups", 
                                      #tokenizer = "words", sentiment_lexicon = "afinn", language = "english", udmodel_lang = #udmodel_lang, remove_numbers = NULL, compound_nouns = TRUE)

sotu_firsts_network <- CreateTextnet(sotu_firsts_nouns)

```
```{r}
#VisTextNet(sotu_firsts_network,label_degree_cut = 0)
VisTextNetD3(sotu_firsts_network)
```

```{r}
sotu_firsts_communities <- TextCommunities(sotu_firsts_network)
top_words_modularity_classes <- InterpretText(sotu_firsts_network, sotu_firsts_nouns)
text_centrality <- TextCentrality(sotu_firsts_network)
```


#4.3 If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively? Please limit your answer to 8 images and 500 words


